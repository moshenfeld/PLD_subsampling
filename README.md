### Privacy Loss Distribution (PLD) Subsampling

This project implements and evaluates privacy amplification by subsampling for Privacy Loss Distribution (PLD) Probability Mass Functions (PMFs).

### Project overview

Differential Privacy (DP) provides a mathematical framework for measuring the privacy guarantees of algorithms. Privacy Loss Distribution (PLD) is a technique for tracking these guarantees through complex compositions of privacy mechanisms.

This project focuses on implementing and testing the subsampling transformation for PLD PMFs, which represents the privacy amplification effect when data is randomly subsampled before applying a differentially private mechanism.

### Key components

- `subsample_pld_pmf.py`: Implements the transformation of a Privacy Loss Distribution PMF using subsampling.
- `pmf_utils.py`: Utilities for creating PMFs on a chosen discretization.
- `pmf_compare.py`: Comparison metrics (Wasserstein-1 distance).
- `test_utils.py`: Analytical formulas and experiment runner (builds PMFs, computes W1 and epsilons).
- `plot_utils.py`: Plotting utilities that return figures (CDF and epsilon-vs-delta).
- `main.py`: Runs experiments and saves figures under `plots/`.
 - `plots/`: Generated figures (now gitignored).

### Quickstart

1) Create a virtual environment and install dependencies

```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

2) Run the tests/demo

```bash
python main.py
```

3) Plots are generated by `python main.py` and saved under `plots/` (this directory is ignored by git).

### Repository hygiene

- The `plots/` directory is treated as a build artifact and is ignored by git. This keeps the repository lightweight while still allowing you to generate figures locally. If you want to version a subset of figures, consider exporting them elsewhere (e.g., `examples/figures/`) or using Git LFS.

### Implementation details

The core implementation in `subsample_pld_pmf.py` transforms a source PMF using the subsampling transformation formula:

```
p'_i = sum_{j=I(l_{i-1})+1}^{I(l_i)} ((1-q)e^{-l_j} + q) p_j
```

Where:
- `q` is the sampling probability
- `l_j` are the loss values
- `p_j` are the probabilities
- `I(l)` is defined via the threshold mapping `ln(1 + (exp(l) - 1)/q)`

### Current results

- We compute Wasserstein-1 (W1) between our PMF and the library PMF and compare analytical vs. library vs. our epsilons for a range of deltas. Run `python main.py` to see tables and plots.

### Dependencies

See `requirements.txt` for a pinned, known-good set of versions. Top-level dependencies are:
- NumPy
- SciPy
- Matplotlib
- dp-accounting (Google's Differential Privacy library)